Web Scraping/Parsing გამოიყენება ვებ გვერდებიდან დიდი ინფორმაციის წამოსაღებად და
წასაკითხად ავტომატურ რეჟიმში. Scraping ნიშნავს ინფორმაციის წამოღებას, ხოლოდ parsing
ნიშნავს წამოღებული ინფორმაციის დაყოფას ნაწილებად და საჭირო კონტენტზე წვდომას.
✲ Web Scraping გამოიყენება სხვადასხვა მიზნებისთვის. მაგ. მარკეტინგისთვის, როცა კომპანიას

აინტერესებს პროდუქტების შესახებ ფასების შედარება სხვადასხვა საიტზე, სოციაულურ ვებ-
გვერდებიდან ინფორმაციის ამოსაკრებად (მაგ. ტვიტების ანალიზი). თუმცა ამ დროს

გასათვალისწინებელია ლეგალური და ეთიკური ასპექტები.
✲ ვებ გვერდზე განთავსებული ინფორმაცია არ არის სტრუქტურირებული. იმ შემთხვევაში, თუ
გვინდა ინფორმაცია წამოვიღოთ სტრუქტურირებული სახით, მაშინ ვიყენებთ ვებ სერვისებს,
API-ს.
✲ Python-ში შეგიძლიათ გამოიყენოთ შემდეგი framework-ები საიტიდან მონაცემების წამოსაღებად:
1. Beautiful Soup –ის მეშვეობით შესაძლებელია html ან xml კოდის პარსინგი, დამუშავება და
სასურველი ინფორმაციის წამოღება
2. Selenium - გარდა html-ისა აქვს Javascript-ის scraping-ის მხარდაჭერა.
3. Scrapy - გამოიყენება ძალიან დიდი მონაცემების წამოსაღებად ასინქრონულად
პარალელურ რეჟიმში (აქვს multithreading-ის - მრავალნაკადიანი პროგრამირების
მხარდაჭერა), ავტომატურად ახდენს საიტის ერთი ლინკიდან ამავე საიტის სხვა
ლინკებზე გადასვლას (Crawling) – (კომპლექსური Crawling-ის მაგალითია Google Search
Engine)
✲ დოკუმენტაცია: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
✲ ინსტალაცია: pip install beautifulsoup4 ან pip install bs4
✲ ეტაპები:
1. შეარჩიეთ საჭირო URL, საიდანაც გსურთ ინფორმაციის წამოღება
2. დაათვალიერეთ ლინკის source code (inspect element-ით); მოძებნეთ ის ინფორმაცია
რომლის წამოღებაც გსურთ საიტიდან
3. დაწერეთ კოდი და გაუშვით, რომელიც წამოიღებს ინფორმაციას საიტიდან და
დაამუშავებს მას
4. შეინახეთ წამოღებული ინფორმაცია საჭირო ფორმატში

თავდაპირველად შეარჩიეთ ვებ საიტი საიდანაც გსურთ ინფორმაციის წამოღება. გაითვალისწინეთ, რომ შესაძლებელია
საიტს ქონდეს robots.txt ფაილი, სადაც მითითებული იმ ქვე გვერდების ლინკები, რომლებზე წვდომაც ნებადართულია.
robots.txt ფაილი უმეტესად გამოიყენება search engine-ებისთვის.
✲ ვებ გვერდიდან ინფორმაციის წამოსაღებად საჭიროა ვებ საიტის კონკრეტული გვერდის URL მიუთითოთ კოდში და
წამოიღოთ ამ გვერდის html კოდი. ამისათვის უნდა გამოიყენოთ requests მოდული; pip list ბრძანების გამოყენებით
შეგიძლიათ ნახოთ არის თუ არა requests მოდული დაყენებული; თუ არ გაქვთ, დააყენეთ შემდეგი ბრძანების
გამოყენებით pip install requests
✲ Requests მოდულის დოკუმენტაცია: https://requests.readthedocs.io/en/master/
✲ requests მოდულში არის get() ფუნქცია, რომელსაც პარამეტრად გადაეცემა url და გვიბრუნებს პასუხად http სტატუსის
კოდს. მაგ. <Response [200]> პასუხი ნიშნავს, რომ წარმატებით დაუკავშირდა სერვერს. შესაძლებელია სერვერიდან
დაბრუნდეს სხვა პასუხი: მაგ. 301. 404, 502
✲ Response-ის კოდზე წვდომა შესაძლებელია status_code-ის გამოყენებით (იხილეთ ქვემოთ მოცემულ კოდში)
✲ დაბრუნებულ ობიექტს შეგვიძლია მივმართოთ text ატრიბუტით, რომელიც გვიბრუნებს საიტის html კოდს.
სანამ გადახვალთ html კოდის დამუშავებაზე, მანამდე უნდა გაარჩიოთ საიტის source
კოდი inspect element-ის გამოყენებით (F12 ღილაზე დაჭერით გამოდის).
✲ უნდა დაათვალიეროთ თუ როგორ არის აწყობილი საიტი და რა ტეგებია
გამოყენებული. მოძებნეთ თქვენთვის საჭირო არეალი საიტის html კოდში და
საჭირო ბლოკები რომლებიც უნდა წაიკითხოთ კოდში.
საიტიდან წამოღებული კონტენტის დასამუშავებლად გამოიყენება Beautiful Soup
მოდული - იგივეა რაც bs4; მასში აღწერილია BeatifulSoup კლასი, რომლის
დაიმპორტებაა საჭირო, რომ გამოვიყენოთ შესაბამისი ბრძანებები. დაიმპორტება
შესაძლებელია შემდეგი ბრძანებით
✲ BeatifulSoup კლასის გამოყენებით უნდა შეიქმნას ობიექტი, რომლის საშუალებითაც
დავამუშავებთ html კოდს.
პარსინგის დროს საჭიროა soup ტექსტში (რომელშიც მოთავსებულია html კოდი) მოვძებნოთ
ჩვენთვის საჭირო ნაწილი (ტეგი) და მასში არსებული ინფორმაცია. ამისათვის გამოიყენება
ძებნის ფუნქციები: find(), find_all() და სხვა.
✲ find() ფუნქცია აბრუნებს მხოლოდ ერთ მნიშვნელობას, პირველივე შემხვედრს.
ხშირად საჭიროა პარსირებული ინფორმაციის შენახვა ფაილში ან ბაზაში. უმეტესად ესეთ
ინფორმაციას ინახავენ csv ფაილის ფორმატში. ამისათვის შეგვიზლია გამოვიყენოთ
სტანდართული ფაილში ჩაწერის ოპერაციები (write) ან არსებობს ბიბლითეკები csv ფაილთან
სამუშაოდ (მაგ. ბიბლიოთეკა csv - განხილულია მომდევნო სლაიდზე).
ხშირად პარსინგის დროს საჭიროა საიტიდან სურათის წამოღება შესაბამის ფორმატში (მაგ. jpeg,
png, gif). ამ შემთხვევაში უნდა ვიცოდეთ სურათის ლინკი და ანალოგიურად ვახდენთ მის
წამოღებას requests.get() ფუნქციის მეშვეობით.
✲ იმისათვის რომ მივიღოთ სურათის შეესაბამისი კონტენტი .text ატრიბუტის ნაცვლად ვიყენებთ
content ატრიბუტს, რომელიც აბრუნებს ლინკის შიგთავსს ბინარული კოდის სახით. text აბრუნებს
ტექსტური მონაცემის სახით.
✲ ფაილის სახით შენახვისთვის, უნდა გახსნათ ფაილი ჩაწერის რეჟიმში, მაგრამ ვინაიდან არის
ბინარული ფაილი, ფაილის გახსნის რეჟიმად უნდა მივუთითოთ ‘wb’
